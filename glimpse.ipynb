{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9cbf2cd",
   "metadata": {},
   "source": [
    "# Taking a quick look on the data\n",
    "\n",
    "The objective is to quickly gather a series of information on the data we are looking at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2abbfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from anytree import Node, RenderTree\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f3acb0",
   "metadata": {},
   "source": [
    "What are the shapes of the data we are working? \n",
    "- We are assuming (30000, 31) for train data and (300, 1259) for target data\n",
    "- Since test dataset is not labeled we will most likely use a test set and dev set taken from the training set. Selected data should be taken with a specific sampling technique that we will discuss later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc395e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are working with 2000 training examples\n",
      "We are working with 150 test examples\n"
     ]
    }
   ],
   "source": [
    "training_dataset = './data/train_data/*'\n",
    "test_dataset = './data/test_data/*'\n",
    "\n",
    "\n",
    "# Access training ids\n",
    "training_paths = glob(training_dataset)\n",
    "training_ids = [path.split('/')[-1] for path in training_paths]\n",
    "training_ids = [path.split('\\\\')[-1] for path in training_ids]\n",
    "\n",
    "# Access test ids\n",
    "test_paths = glob(test_dataset)\n",
    "test_ids = [path.split('/')[-1] for path in test_paths]\n",
    "test_ids = [path.split('\\\\')[-1] for path in test_ids]\n",
    "\n",
    "print(f'We are working with {len(training_ids)} training examples')\n",
    "print(f'We are working with {len(test_ids)} test examples')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eecfdc",
   "metadata": {},
   "source": [
    "Let us take a look on what is inside each id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa615428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['receiver_data_src_1.npy',\n",
       " 'receiver_data_src_150.npy',\n",
       " 'receiver_data_src_225.npy',\n",
       " 'receiver_data_src_300.npy',\n",
       " 'receiver_data_src_75.npy',\n",
       " 'vp_model.npy']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sample = os.listdir(training_paths[0])\n",
    "training_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef8519c",
   "metadata": {},
   "source": [
    "--- \n",
    "Let us confirm all data has the appropiate shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9aa7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiver data shape: (10001, 31)\n",
      "Target data shape: (300, 1259)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(training_paths)):\n",
    "    assert np.load(os.path.join(training_paths[i], f'receiver_data_src_1.npy')).shape == (10001, 31)\n",
    "    assert np.load(os.path.join(training_paths[i], f'receiver_data_src_75.npy')).shape == (10001, 31)\n",
    "    assert np.load(os.path.join(training_paths[i], f'receiver_data_src_150.npy')).shape == (10001, 31)\n",
    "    assert np.load(os.path.join(training_paths[i], f'receiver_data_src_225.npy')).shape == (10001, 31)\n",
    "    assert np.load(os.path.join(training_paths[i], f'receiver_data_src_300.npy')).shape == (10001, 31)\n",
    "    assert np.load(os.path.join(training_paths[i], f'vp_model.npy')).shape == (300, 1259)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e599891a",
   "metadata": {},
   "source": [
    "---\n",
    "Let us find out the data type of every file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "88d62b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dtype of a training instance is: float32\n",
      "The dtype of a training label is: float64\n"
     ]
    }
   ],
   "source": [
    "training_sample = np.load(os.path.join(training_paths[0], f'receiver_data_src_1.npy'))\n",
    "target_sample = np.load(os.path.join(training_paths[0], f'vp_model.npy'))\n",
    "print(f'The dtype of a training instance is: {training_sample.dtype}')\n",
    "print(f'The dtype of a training label is: {target_sample.dtype}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac33171",
   "metadata": {},
   "source": [
    "Training instances are single float precision, while labels are expected to be double float precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1fd6b9",
   "metadata": {},
   "source": [
    "---\n",
    "Before comencing exploratory data analysis we must decide strategy on train/test split <br>\n",
    "While ThinkOnward provides test and train data I will try to engage in a different strategy that will allow us a much easier understanding of the model performance "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
